{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[enwiki-20150112-400-r10-105752.txt.bz2](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r10-105752.txt.bz2)は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパス[enwiki-20150112-400-r100-10576.txt.bz2](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r100-10576.txt.bz2)を用いよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイル解凍\n",
    "- bzip2 -d (ファイルが上書きされてしまう)\n",
    "- bunzip2 -k (元のファイルを残す)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f 'work/enwiki-20150112-400-r100-10576.txt' ]; then\n",
    "    bunzip2 -k 'data/enwiki-20150112-400-r10-105752.txt.bz2'\n",
    "    bunzip2 -k 'data/enwiki-20150112-400-r100-10576.txt.bz2'\n",
    "    mv 'data/enwiki-20150112-400-r10-105752.txt' 'work'\n",
    "    mv 'data/enwiki-20150112-400-r100-10576.txt' 'work'\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルサイズ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 サンプリング : 710.0323333740234 Mb\n",
      "1/100 サンプリング : 70.12497138977051 Mb\n"
     ]
    }
   ],
   "source": [
    "size10 = os.path.getsize('work/enwiki-20150112-400-r10-105752.txt')\n",
    "size100 = os.path.getsize('work/enwiki-20150112-400-r100-10576.txt')\n",
    "print('1/10 サンプリング : {} Mb'.format(size10 / 1024 / 1024))\n",
    "print('1/100 サンプリング : {} Mb'.format(size100 / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中身確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions, but that several authors have defined as more specific institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary, or harmful. While anti-statism is central, anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.\n",
      "\n",
      "...\n",
      "\n",
      "Stanley Hall has a picturesque external form which reflects the stages of its construction and particular interests of its former owners, and contains some finely crafted internal elements. It is a fine example of the work of Brisbane architect GHM Addison.\n",
      "The place has a special association with the life or work of a particular person, group or organisation of importance in Queensland&apos;s history.\n",
      "Since 1926, the place has been associated with the work of the Catholic Church in female secondary education in Queensland.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 'work/enwiki-20150112-400-r100-10576.txt'\n",
    "!echo '\\n...\\n'\n",
    "!tail -n 5 'work/enwiki-20150112-400-r100-10576.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "\n",
    "+ トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    "+ 空文字列となったトークンは削除  \n",
    "\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    del_token = '.,!?;:()[]\\'\"'\n",
    "    return ' '.join(i.strip(del_token) for i in text.strip().split() if i.strip(del_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(input_file, output_file):\n",
    "    # ファイルがすでに存在するか確認\n",
    "    if not os.path.isfile(output_file):\n",
    "        \n",
    "        with open(input_file) as fi, open(output_file, \"w\") as fo:\n",
    "            \n",
    "            # ファイル各行ごとにtokenize\n",
    "            for text in fi:\n",
    "                tokens = tokenizer(text)\n",
    "                \n",
    "                # 空文字でなければ書き込む\n",
    "                if tokens:\n",
    "                    print(tokens, file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_file('work/enwiki-20150112-400-r100-10576.txt', 'work/r100corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\n",
      "As a subtle and anti-dogmatic philosophy anarchism draws on many currents of thought and strategy Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy There are many types and traditions of anarchism not all of which are mutually exclusive Anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications Anarchism is usually considered a radical left-wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 'work/r100corpus.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．  \n",
    "\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[使った国名リスト](http://www.jal.co.jp/5931/readme/code.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.jal.co.jp/5931/readme/code.html\"\n",
    "table = pd.read_html(url)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 国名リストの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>国/地域名</td>\n",
       "      <td>英語名</td>\n",
       "      <td>国/地域名コード</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>アイスランド</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>ISL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>アイルランド</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>IRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>アゼルバイジャン</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>AZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>アフガニスタン</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0            1         2\n",
       "0     国/地域名          英語名  国/地域名コード\n",
       "1    アイスランド      Iceland       ISL\n",
       "2    アイルランド      Ireland       IRL\n",
       "3  アゼルバイジャン   Azerbaijan       AZE\n",
       "4   アフガニスタン  Afghanistan       AFG"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2語以上からなる国名の (old, new) リストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_replace_country(countries):\n",
    "    for country in countries:\n",
    "        old = tokenizer(country)\n",
    "        new = old.replace(' ', '_')\n",
    "        length = len(old.split())\n",
    "        \n",
    "        if length > 1:\n",
    "            yield (length, old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 長さで降順にソート\n",
    "replace_countries = [(old, new) for _, old, new in sorted(iter_replace_country(table[1][1:]), key=lambda x: -x[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上のリストでreplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_country(text):\n",
    "    for old, new in replace_countries:\n",
    "        text = text.replace(old, new)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_country_file(input_file, output_file):\n",
    "    # ファイルがすでに存在するか確認\n",
    "    if not os.path.isfile(output_file):\n",
    "        \n",
    "        with open(input_file) as fi, open(output_file, 'w') as fo:\n",
    "            for text in fi:\n",
    "                fo.write(combined_country(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'New Zealand, United States, South Africa, Saudi Arabia, United States, Papua New Guinea and Isle of Man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Zealand, United States, South Africa, Saudi Arabia, United States, Papua New Guinea and Isle of Man\n",
      "New Zealand United States South Africa Saudi Arabia United States Papua New Guinea and Isle of Man\n",
      "New_Zealand United_States South_Africa Saudi_Arabia United_States Papua_New_Guinea and Isle_of_Man\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(tokenizer(test))\n",
    "print(combined_country(tokenizer(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_country_file('work/r100corpus.txt', 'work/r100corpus_combined_country.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term anarchism is a compound word composed from the word anarchy and the suffix -ism themselves derived respectively from the Greek i.e anarchy from anarchos meaning one without rulers from the privative prefix ἀν- an- i.e without and archos i.e leader ruler cf archon or arkhē i.e authority sovereignty realm magistracy and the suffix or -ismos -isma from the verbal infinitive suffix -ίζειν -izein The first known use of this word was in 1539.\"Anarchist was the term adopted by Maximilien de Robespierre to attack those on the left whom he had used for his own ends during the French Revolution but was determined to get rid of though among these anarchists there were few who exhibited the social revolt characteristics of later anarchists There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation such as William Godwin and Wilhelm Weitling but they did not use the word anarchist or anarchism in describing themselves or their beliefs Pierre-Joseph Proudhon was the first political philosopher to call himself an anarchist marking the formal birth of anarchism in the mid-nineteenth century Since the 1890s from France the term libertarianism has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United_States its use as a synonym is still common outside the United_States On the other hand some use libertarianism to refer to individualistic free-market philosophy only referring to free-market anarchism as libertarian anarchism\n",
      "The anti-authoritarian sections of the First International were the precursors of the anarcho-syndicalists seeking to replace the privilege and authority of the State with the free and spontaneous organisation of labour In 1886 the Federation of Organized Trades and Labor Unions FOTLU of the United_States and Canada unanimously set 1 May 1886 as the date by which the eight-hour work day would become standard\n",
      "In response unions across the United_States prepared a general strike in support of the event On 3 May in Chicago a fight broke out when strikebreakers attempted to cross the picket line and two workers died when police opened fire upon the crowd The next day 4 May anarchists staged a rally at Chicago's Haymarket Square A bomb was thrown by an unknown party near the conclusion of the rally killing an officer In the ensuing panic police opened fire on the crowd and each other Seven police officers and at least four workers were killed Eight anarchists directly and indirectly related to the organisers of the rally were arrested and charged with the murder of the deceased officer The men became international political celebrities among the labour movement Four of the men were executed and a fifth committed suicide prior to his own execution The incident became known as the Haymarket affair and was a setback for the labour movement and the struggle for the eight-hour day In 1890 a second attempt this time international in scope to organise for the eight-hour day was made The event also had the secondary purpose of memorializing workers killed as a result of the Haymarket affair Although it had initially been conceived as a once-off event by the following year the celebration of International Workers Day on May Day had become firmly established as an international worker's holiday\n",
      "grep: write error: Broken pipe\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cat 'work/r100corpus_combined_country.txt' | grep 'United_States' | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語$t$に関して，単語$t$と文脈語$c$のペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．  \n",
    "\n",
    "- ある単語$t$の前後$d$単語を文脈語$c$として抽出する（ただし，文脈語に単語$t$そのものは含まない）  \n",
    "- 単語$t$を選ぶ度に，文脈幅$d$は{${1, 2, 3, 4, 5}$}の範囲でランダムに決める．  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context(text):\n",
    "    terms = text.rstrip(\"\\n\").split()\n",
    "                \n",
    "    # termごとにdを1~5からランダムに決め、contextを取り出す\n",
    "    for i, t in enumerate(terms):\n",
    "        d = random.randint(1,5)\n",
    "        context_terms = terms[max(0,i-d) : i] + terms[i+1 : i+1+d]\n",
    "        \n",
    "        for c in context_terms:\n",
    "            yield t, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_file(input_file, output_file):\n",
    "    # ファイルがすでに存在するか確認\n",
    "    if not os.path.isfile(output_file):\n",
    "        \n",
    "        with open(input_file) as fi, open(output_file, 'w') as fo:\n",
    "            for text in fi:\n",
    "                for t, c in extract_context(text):\n",
    "                    print('{}\\t{}'.format(t, c), file=fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Minecraft is a dangerous game that causes time sense to go wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minecraft is\n",
      "Minecraft a\n",
      "Minecraft dangerous\n",
      "is Minecraft\n",
      "is a\n",
      "is dangerous\n",
      "a Minecraft\n",
      "a is\n",
      "a dangerous\n",
      "a game\n"
     ]
    }
   ],
   "source": [
    "for t, c in islice(extract_context(test), 10):\n",
    "    print(t, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_context_file('work/r100corpus_combined_country.txt', 'work/r100context.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\n",
      "Anarchism\ta\n",
      "Anarchism\tpolitical\n",
      "Anarchism\tphilosophy\n",
      "Anarchism\tthat\n",
      "is\tAnarchism\n",
      "is\ta\n",
      "is\tpolitical\n",
      "is\tphilosophy\n",
      "is\tthat\n"
     ]
    }
   ],
   "source": [
    "!head 'work/r100context.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68086580 work/r100context.txt\n"
     ]
    }
   ],
   "source": [
    "!wc 'work/r100context.txt' -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- $f(t,c)$: 単語$t$と文脈語$c$の共起回数\n",
    "- $f(t,*)$: 単語$t$の出現回数\n",
    "- $f(*,c)$: 文脈語$c$の出現回数\n",
    "- $N$: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_occur_freq(input_file):\n",
    "    f_tc = defaultdict(int)\n",
    "    f_t = defaultdict(int)\n",
    "    f_c = defaultdict(int)\n",
    "    N = 0\n",
    "    \n",
    "    with open(input_file) as fi:\n",
    "        prev_t = ''\n",
    "        for line in tqdm(fi):\n",
    "            t, c = line.rstrip('\\n').split('\\t', 1)\n",
    "\n",
    "            # f(t, c), f(c), Nを記録\n",
    "            f_tc[(t, c)] += 1\n",
    "            f_t[t] += 1\n",
    "            f_c[c] += 1\n",
    "            N += 1\n",
    "            \n",
    "    return f_tc, f_t, f_c, N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickleで保存\n",
    "- jsonはkeyをtupleで保存できない\n",
    "    - {('Anarchism', 'is') : 10} は不可\n",
    "- 工夫が必要\n",
    "    - {'key' : ('Anarchism', 'is'), 'value' : 10}\n",
    "        - keyとvalueで分ける\n",
    "    - {'Anarchism is' : 10}\n",
    "        - スペースまたはタブで区切る\n",
    "- pickleの場合\n",
    "    - keyがtupleでも可\n",
    "    - defaultdictの引数にlambdaを使うとエラー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_co_occur_freq_file(input_file, output_file, overwrite=False):\n",
    "    # ファイルがすでに存在するか確認\n",
    "    if not os.path.isfile(output_file) or overwrite:\n",
    "        \n",
    "        with open(output_file, 'wb') as fo:\n",
    "            # 各それぞれを計算\n",
    "            f_tc, f_t, f_c, N = get_co_occur_freq(input_file)\n",
    "\n",
    "            # ファイルに保存\n",
    "            print('saving the file now...')\n",
    "            pickle.dump({'f_tc':f_tc, 'f_t':f_t, 'f_c':f_c, 'N':N}, fo)\n",
    "            print('complete! The file was saved to \"{}\"'.format(output_file))\n",
    "            \n",
    "    else:\n",
    "        print('\"{}\" already exist.'.format(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"work/r100freq.pickle\" already exist.\n"
     ]
    }
   ],
   "source": [
    "write_co_occur_freq_file('work/r100context.txt' ,'work/r100freq.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑2分50秒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memo\n",
    "- メモリを消費しない為には\n",
    "    - 82のファイルを単語でソート\n",
    "    - ソートするときはファイルを分割してマージソート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列$X$を作成せよ．ただし，行列$X$の各要素$X_{tc}$は次のように定義する．\n",
    "\n",
    "- $f(t, c) \\geq 10$ならば，$X_{tc} = {\\rm PPMI}(t, c) = \\max\\{\\log \\frac{N \\times f(t,c)}{f(t,*) \\times f(*,c)}, 0\\}$\n",
    "- $f(t, c) < 10$ならば，$X_{tc} = 0$  \n",
    "\n",
    "ここで，${\\rm PPMI}(t, c)$はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列$X$の行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列$X$のほとんどの要素は$0$になるので，非$0$の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_file = 'work/r100_compressed_matrix.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(matrix_file):\n",
    "    # 読み込み\n",
    "    with open('work/r100freq.pickle', 'rb') as fi:\n",
    "        freq_dict = pickle.load(fi)\n",
    "\n",
    "    f_tc = freq_dict['f_tc']\n",
    "    f_t = freq_dict['f_t']\n",
    "    f_c = freq_dict['f_c']\n",
    "    N = freq_dict['N']\n",
    "    X = sparse.lil_matrix((len(f_t), len(f_c)))\n",
    "    word2index = defaultdict(lambda:len(word2index))\n",
    "\n",
    "    # matrix作成\n",
    "    for t, c in f_tc:\n",
    "        if f_tc[(t, c)] >= 10:\n",
    "            ppmi = max(np.log(N * f_tc[(t, c)] / (f_t[t] * f_c[c])), 0)\n",
    "            X[word2index[t], word2index[c]] = ppmi\n",
    "            \n",
    "    # word2indexを保存\n",
    "    with open('work/word2index.json', 'w') as fo:\n",
    "        json.dump(word2index, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383349, 383349)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.58390874, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.58207298, 0.        , 0.28614027, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.30470806, 1.38357106, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(X.shape)\n",
    "# X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memo\n",
    "scipy.sparse : 種類によって、structureが違う  \n",
    "- coo, lil, dok : 1つづつ要素を入力するのに便利、算術演算は不可or遅い  \n",
    "- csc, csr : 算術演算は早い、要素挿入などは遅い  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [coo](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix)\n",
    "- [lil](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix)\n",
    "- [dok](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html#scipy.sparse.dok_matrix)\n",
    "- [csc](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix)\n",
    "- [csr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(matrix_file):\n",
    "    # SVDで300dimに次元圧縮\n",
    "    compressed_matrix = TruncatedSVD(300).fit_transform(X)\n",
    "    \n",
    "    # 保存\n",
    "    with open(matrix_file, 'wb') as fo:\n",
    "        pickle.dump(compressed_matrix, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383349, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 10.8240754 ,  -9.74378182,  13.94872879, ...,   0.42175619,\n",
       "         -0.13219345,  -0.52398254],\n",
       "       [ 13.31035638, -10.96632689,  21.68599975, ...,  -0.21242377,\n",
       "          0.31986328,   0.05241682],\n",
       "       [  2.88213064,  -2.34753214,   0.50170894, ...,   0.41869774,\n",
       "         -1.16036486,   1.22787941],\n",
       "       ...,\n",
       "       [  0.        ,  -0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,  -0.        ],\n",
       "       [  0.        ,  -0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,  -0.        ],\n",
       "       [  0.        ,  -0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,  -0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(compressed_matrix.shape)\n",
    "compressed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(matrix_file, 'rb') as fi:\n",
    "    matrix = pickle.load(fi)\n",
    "    \n",
    "with open('work/word2index.json') as fi:\n",
    "    word2index = json.load(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ word2indexは f_tc >= 10 の 値のみを保持しているため、word数が少ない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix.shape = (383349, 300)\n",
      "len(word2index) = 34073\n"
     ]
    }
   ],
   "source": [
    "print('matrix.shape = {}'.format(matrix.shape))\n",
    "print('len(word2index) = {}'.format(len(word2index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.67042754e+00, -7.97679639e-01,  4.83658640e-01,  6.46441761e+00,\n",
       "        2.33432846e+00, -1.20769317e+00, -2.31089079e+00,  3.48054571e-01,\n",
       "        2.97345403e-02,  5.00593945e-01, -2.47616574e-01,  1.26203295e+00,\n",
       "       -2.59763758e+00, -2.64483439e+00,  2.46400364e+00,  2.34843206e+00,\n",
       "       -3.29193245e+00,  6.85065832e-01,  2.67604669e+00,  4.55965011e-01,\n",
       "       -1.23002647e-01,  7.40697015e-01,  1.07810134e-01, -1.06869851e+00,\n",
       "        2.90179984e-01, -1.61695332e+00,  9.59575152e-01,  3.65895272e+00,\n",
       "        6.08389216e-01, -4.31464005e+00, -8.07831242e-01,  1.77890964e-01,\n",
       "       -8.89742180e-02,  1.15314676e+00,  9.68645229e-01, -4.88669318e-01,\n",
       "        3.02646083e+00,  6.52261726e-01,  1.76830870e+00,  1.30764205e+00,\n",
       "        1.24047947e-03, -8.84145208e-01, -2.55120934e+00, -1.43524336e+00,\n",
       "        4.14247851e-01, -3.80612089e-01, -4.16792161e-02,  9.13717867e-01,\n",
       "        2.30521373e-01,  9.88985700e-01,  1.91767517e+00, -2.06546754e-01,\n",
       "        1.15842955e+00, -2.14359693e-01,  9.02461749e-01, -1.15891812e-01,\n",
       "        1.98352213e+00,  8.76621855e-01, -6.43904017e-01,  7.06743184e-01,\n",
       "       -1.47511818e+00,  1.32163382e+00,  1.07763888e+00,  1.88429082e+00,\n",
       "       -9.02395110e-01,  9.96152730e-01,  7.81353093e-02,  1.17405876e-01,\n",
       "       -9.37832497e-01,  1.99550775e+00,  2.77991867e+00,  1.13520154e+00,\n",
       "        1.75094567e-01,  2.74861634e-01, -1.21225931e+00,  2.37193833e+00,\n",
       "        7.63653528e-02,  4.59678687e-01, -2.65860630e-01,  1.40720016e+00,\n",
       "       -3.69469540e-01,  4.25198718e-01, -2.08581543e+00,  5.87646259e-01,\n",
       "       -7.81412529e-01, -3.99281097e-01,  3.14424018e-01, -6.70856740e-01,\n",
       "        5.09326609e-01, -1.43591809e-01, -5.47122636e-02,  6.14750199e-02,\n",
       "       -1.13891004e-01, -5.59079861e-01,  2.65729421e+00, -1.20300783e+00,\n",
       "        4.04634352e-01, -1.59400806e-01, -2.84953137e-01, -7.80026158e-01,\n",
       "        3.00597991e-01,  1.33124425e-01,  4.71054780e-01, -9.09262887e-02,\n",
       "       -1.53499141e+00,  9.35807731e-01, -3.23647365e-02,  4.64135457e-02,\n",
       "        1.42149741e+00,  1.07793911e+00, -1.85220146e+00, -1.92600602e-01,\n",
       "        8.46693369e-01, -6.73818041e-01, -1.11789458e+00, -4.24847918e-01,\n",
       "        1.38722955e-01, -7.86132521e-02,  3.00086932e-01, -9.43031713e-01,\n",
       "       -6.78025700e-01, -1.88204989e-01, -9.18400044e-01,  1.50641090e-01,\n",
       "        7.95300519e-01, -4.35790709e-01,  5.20586071e-01, -8.82170882e-02,\n",
       "        1.70210501e+00, -8.88786063e-01, -8.07211193e-01, -1.74519811e+00,\n",
       "        4.69850448e-01,  2.96808381e-01, -2.15468045e-01,  4.30420780e-01,\n",
       "       -2.43661035e+00, -7.99968461e-01, -1.86922631e+00,  3.70612173e-01,\n",
       "        2.10877317e+00, -1.20900946e+00,  2.03071392e+00, -8.16999887e-02,\n",
       "        1.65962000e-01, -3.59658949e-01,  5.73348449e-01,  1.05442091e+00,\n",
       "        1.03728296e+00, -5.20779770e-01, -9.43024011e-01, -6.58411507e-02,\n",
       "       -4.14408663e-01, -1.87726653e-01,  1.46742397e+00, -3.50703845e-01,\n",
       "        2.62848809e-01,  1.29046461e+00,  4.46862638e-01,  3.67936058e-01,\n",
       "       -1.17711474e+00,  5.10442907e-01, -8.95464596e-01,  2.63130032e-01,\n",
       "        2.18206788e-01,  9.27215238e-01,  1.75484163e+00, -2.26633649e+00,\n",
       "       -1.12077016e+00, -1.28260258e+00, -3.39057596e-01,  6.27003234e-01,\n",
       "        1.39805797e+00,  7.39605571e-01,  2.23083072e+00,  1.97944401e+00,\n",
       "        4.52395133e-01,  6.75676411e-01, -9.69353855e-01, -1.27802434e+00,\n",
       "       -1.63852069e+00,  9.81082090e-01, -1.16898179e+00, -8.58683133e-01,\n",
       "       -1.74078303e+00,  6.16910574e-01, -4.19892512e-01, -1.02859520e+00,\n",
       "       -1.43838936e+00,  3.47352876e-01, -9.55231899e-02, -8.65730398e-01,\n",
       "        8.96327381e-02,  6.39934351e-01,  9.63257635e-01,  7.51471151e-01,\n",
       "        4.11296154e-01,  5.80645723e-01,  7.62222975e-01, -6.65834598e-02,\n",
       "       -2.29376213e-03,  1.05185068e-01, -7.19938125e-01,  4.58119330e-01,\n",
       "       -2.85194625e-02,  2.00775105e+00, -1.14070324e+00,  4.48852448e-01,\n",
       "       -1.13330947e+00,  1.04453999e+00, -6.77873389e-01,  2.20588233e-01,\n",
       "       -1.49579217e+00,  9.55552351e-01,  3.78854939e-01,  2.21084175e-01,\n",
       "        5.66797771e-01, -9.28363607e-01,  3.04454550e-01,  4.98949338e-01,\n",
       "       -4.80462149e-01, -2.17704990e+00, -2.30399528e-01, -6.18063012e-01,\n",
       "       -7.07682260e-01,  9.27038995e-02,  3.02703757e-01,  6.50736233e-01,\n",
       "       -4.04184690e-01,  8.72662493e-01, -3.01757901e-02,  4.60071544e-01,\n",
       "        6.44463270e-01, -4.25038611e-01,  4.98224929e-01,  6.46081526e-01,\n",
       "        2.26292422e-01, -3.69252751e-01,  1.35842035e+00, -1.56212782e-01,\n",
       "        2.24019543e-01,  9.00524620e-01, -4.85049982e-01, -3.02101257e-01,\n",
       "       -1.93100804e-01,  4.67295352e-01, -1.36664812e-01, -8.19226290e-01,\n",
       "        1.77869412e+00,  1.00739767e-01, -3.52211916e-01,  1.75055602e-01,\n",
       "        5.62323771e-01,  2.44390579e-01,  1.22156167e+00, -6.28554732e-02,\n",
       "        9.32254839e-01,  1.47676962e-01, -9.30276775e-01, -7.61833099e-01,\n",
       "       -5.48340272e-01, -6.92820694e-02, -5.73060733e-02, -1.84958944e-01,\n",
       "        6.84330086e-01, -9.49570966e-01, -1.40197168e-01, -5.86100675e-01,\n",
       "        7.87343804e-01, -1.06719792e+00,  7.92445524e-01, -3.89877391e-01,\n",
       "        4.74321686e-01, -3.10479951e-01,  1.55617890e-01,  5.09545508e-01,\n",
       "       -3.21830899e-01,  4.34203940e-01,  7.14641367e-01,  6.08577703e-01,\n",
       "        6.34142539e-01,  7.57252576e-01, -8.74980534e-01, -6.14957066e-01,\n",
       "        8.22650114e-01,  2.10602608e-01,  1.12133878e-01,  2.60305574e-01,\n",
       "        2.61711744e-01,  1.23129497e+00,  5.25747240e-02, -7.00722915e-01,\n",
       "        4.63498831e-02, -3.10424194e-01, -7.31861658e-01, -4.49224448e-02,\n",
       "       -3.58243408e-01, -8.05144995e-03, -2.20797873e-01,  2.14596773e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[word2index['United_States']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 単語の類似度\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2): \n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068498000328898"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = matrix[word2index['United_States']]\n",
    "v2 = matrix[word2index['U.S']]\n",
    "cos_sim(v1, v2)  # vec同士の演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### または"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8068498]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = v1.reshape(1, -1)\n",
    "v2 = v2.reshape(1, -1)\n",
    "cosine_similarity(v1, v2)  # matrix同士の演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 自作cosで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar1_topn(vec, matrix, word2index, start=1, end=11):\n",
    "    word_sim = dict()\n",
    "    for w, idx in word2index.items():\n",
    "        word_sim[w] = cos_sim(matrix[idx], vec)\n",
    "\n",
    "    return {w : s for w, s in islice(sorted(word_sim.items(), key=lambda x: -x[1]), start, end)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scotland': 0.7035333633555668,\n",
       " 'Spain': 0.5949160803484714,\n",
       " 'Australia': 0.5813180337813137,\n",
       " 'Wales': 0.5561339431439319,\n",
       " 'France': 0.5553863308259942,\n",
       " 'Italy': 0.5489628858410673,\n",
       " 'Germany': 0.5370692279123737,\n",
       " 'Ireland': 0.5289999988115132,\n",
       " 'United_Kingdom': 0.5176723042774007,\n",
       " 'Britain': 0.5074796089614494}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar1_topn(matrix[word2index['England']], matrix, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. sklearnのcosで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar2_topn(vec, matrix, word2index, start=1, end=11):\n",
    "    sim_vec = cosine_similarity(vec.reshape(1, -1), matrix)[0]\n",
    "    sim_indices = np.argsort(sim_vec)[::-1][start:end]\n",
    "    index2word = {v : k for k, v in word2index.items()}\n",
    "    \n",
    "    return {index2word[i] : sim_vec[i] for i in sim_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scotland': 0.7035333633555734,\n",
       " 'Spain': 0.5949160803484759,\n",
       " 'Australia': 0.5813180337813166,\n",
       " 'Wales': 0.5561339431439363,\n",
       " 'France': 0.5553863308259964,\n",
       " 'Italy': 0.548962885841071,\n",
       " 'Germany': 0.5370692279123762,\n",
       " 'Ireland': 0.5289999988115169,\n",
       " 'United_Kingdom': 0.5176723042774056,\n",
       " 'Britain': 0.5074796089614539}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar2_topn(matrix[word2index['England']], matrix, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 2の改良ver (演算する行列を絞る)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar3_topn(vec, matrix, word2index, start=1, end=11):\n",
    "    # word2indexに登場するものにmatrixを絞る (次元数 : 38万 -> 3万)\n",
    "    indices = np.array([i for i in word2index.values()])\n",
    "    matrix = matrix[indices]\n",
    "    \n",
    "    # 演算\n",
    "    sim_vec = cosine_similarity(vec.reshape(1, -1), matrix)[0]\n",
    "    sim_indices = np.argsort(sim_vec)[::-1][start:end]\n",
    "    index2word = {v : k for k, v in word2index.items()}\n",
    "    \n",
    "    return {index2word[i] : sim_vec[i] for i in sim_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scotland': 0.7035333633555734,\n",
       " 'Spain': 0.5949160803484759,\n",
       " 'Australia': 0.5813180337813166,\n",
       " 'Wales': 0.5561339431439363,\n",
       " 'France': 0.5553863308259964,\n",
       " 'Italy': 0.548962885841071,\n",
       " 'Germany': 0.5370692279123762,\n",
       " 'Ireland': 0.5289999988115169,\n",
       " 'United_Kingdom': 0.5176723042774056,\n",
       " 'Britain': 0.5074796089614539}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar3_topn(matrix[word2index['England']], matrix, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ くそ早い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = matrix[word2index['Spain']] - matrix[word2index['Madrid']] + matrix[word2index['Athens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spain': 0.9385985291752981,\n",
       " 'France': 0.8538903741649336,\n",
       " 'Sweden': 0.836090676998262,\n",
       " 'Italy': 0.8288329327777637,\n",
       " 'Germany': 0.8027078157007357,\n",
       " 'Netherlands': 0.8025955806250862,\n",
       " 'Austria': 0.8014099452237468,\n",
       " 'Portugal': 0.7770318968921359,\n",
       " 'Télévisions': 0.7711391042620539,\n",
       " 'Denmark': 0.7696654238787894}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar3_topn(vec, matrix, word2index, start=0, end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
